{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal and data import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goal: predict gender of TV viewers based on set of characteristics (time spend on watching 40 different TV genres of and some info about devices)\n",
    "\n",
    "Input data: 2K observations for training (includes gender) and K observation for test (gender is missing and is target for prediction)\n",
    "\n",
    "Variables: 40 numeric variables and 5 categorical {-1;0;1}\n",
    "\n",
    "Missing values: there are no missing values in the training/test dataset\n",
    "\n",
    "Balanced/unbalanced dataset: training data set is quite balanced\n",
    "\n",
    "Outliers – not observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender    0\n",
      "x1        0\n",
      "x2        0\n",
      "x3        0\n",
      "x4        0\n",
      "x5        0\n",
      "x6        0\n",
      "x7        0\n",
      "x8        0\n",
      "x9        0\n",
      "x10       0\n",
      "x11       0\n",
      "x12       0\n",
      "x13       0\n",
      "x14       0\n",
      "x15       0\n",
      "x16       0\n",
      "x17       0\n",
      "x18       0\n",
      "x19       0\n",
      "x20       0\n",
      "x21       0\n",
      "x22       0\n",
      "x23       0\n",
      "x24       0\n",
      "x25       0\n",
      "x26       0\n",
      "x27       0\n",
      "x28       0\n",
      "x29       0\n",
      "x30       0\n",
      "x31       0\n",
      "x32       0\n",
      "x33       0\n",
      "x34       0\n",
      "x35       0\n",
      "x36       0\n",
      "x37       0\n",
      "x38       0\n",
      "x39       0\n",
      "x40       0\n",
      "x41       0\n",
      "x42       0\n",
      "x43       0\n",
      "x44       0\n",
      "x45       0\n",
      "dtype: int64\n",
      "            Gender           x1           x2           x3           x4  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean      0.465500     5.034083     5.018200     4.911232     4.912412   \n",
      "std       0.498933     2.909781     2.889966     2.871402     2.925843   \n",
      "min       0.000000     0.001557     0.006796     0.004259     0.006314   \n",
      "25%       0.000000     2.460645     2.535577     2.503660     2.323453   \n",
      "50%       0.000000     5.059938     5.024768     4.805803     4.806753   \n",
      "75%       1.000000     7.560343     7.571328     7.398937     7.513002   \n",
      "max       1.000000     9.997439     9.998735     9.998568     9.991082   \n",
      "\n",
      "                x5           x6           x7           x8           x9  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean      4.956377     5.024579     4.935966     5.067840     5.086828   \n",
      "std       2.897600     2.913924     2.921489     2.833147     2.850428   \n",
      "min       0.009633     0.002398     0.003581     0.002622     0.008044   \n",
      "25%       2.471000     2.495113     2.334584     2.576546     2.600456   \n",
      "50%       4.924308     5.020893     4.964938     5.193205     5.201639   \n",
      "75%       7.496324     7.484141     7.527601     7.497030     7.510152   \n",
      "max       9.990208     9.997379     9.999543     9.997743     9.996769   \n",
      "\n",
      "          ...               x36          x37          x38          x39  \\\n",
      "count     ...       2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean      ...          4.984694     4.908684     4.868374     4.995217   \n",
      "std       ...          2.931335     2.848105     2.852245     2.911902   \n",
      "min       ...          0.002803     0.000178     0.006052     0.000361   \n",
      "25%       ...          2.426546     2.494812     2.383270     2.489068   \n",
      "50%       ...          5.042467     4.883605     4.776674     5.024223   \n",
      "75%       ...          7.540748     7.327315     7.278360     7.612702   \n",
      "max       ...          9.999721     9.998322     9.987979     9.997630   \n",
      "\n",
      "               x40          x41         x42          x43          x44  \\\n",
      "count  2000.000000  2000.000000  2000.00000  2000.000000  2000.000000   \n",
      "mean      5.165625    -0.026000    -0.00750    -0.016000     0.032000   \n",
      "std       2.803282     0.817715     0.82449     0.814295     0.808271   \n",
      "min       0.002939    -1.000000    -1.00000    -1.000000    -1.000000   \n",
      "25%       2.798774    -1.000000    -1.00000    -1.000000    -1.000000   \n",
      "50%       5.159642     0.000000     0.00000     0.000000     0.000000   \n",
      "75%       7.596508     1.000000     1.00000     1.000000     1.000000   \n",
      "max       9.987218     1.000000     1.00000     1.000000     1.000000   \n",
      "\n",
      "               x45  \n",
      "count  2000.000000  \n",
      "mean      0.012500  \n",
      "std       0.812203  \n",
      "min      -1.000000  \n",
      "25%      -1.000000  \n",
      "50%       0.000000  \n",
      "75%       1.000000  \n",
      "max       1.000000  \n",
      "\n",
      "[8 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "#set directory\n",
    "os.chdir('C:\\\\Users\\\\Inna\\\\Desktop\\\\pydata')\n",
    "\n",
    "#read xlsx\n",
    "df = pd.read_excel('Data_RB_Case_Study.xlsx', sheetname='Train')\n",
    "print (df.isnull().sum()) #no missing values\n",
    "print(df.describe()) #no outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x288d643e550>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#gender distribution\n",
    "sns.countplot(x='Gender', data=df, color=\"teal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Potential overfitting problem – too many variables for provided dataset. 50 variables (40 as time per genre+10 comes as dummy) and only 2000 obs.\n",
    "\n",
    "Several approaches to select/reduce number of variables:\n",
    "\n",
    "Filtering (using heatmap of Pearson correlations to select most correlated variables)\n",
    "Recursive feature elimination – build a lot of models and add/remove feature step by step to select most important variables\n",
    "Embedded (Lasso regression) – set penalties for coefficients and optimize them with penalty parameter abs(lambda)\n",
    "PCA – reduce dimensions of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22a7fae0ba8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADwdJREFUeJzt3X+s3Xddx/Hny9YLyK+guzhsKy3QhBSCkF3mr4gLsKRobAnOpCMqS9CirE6HZg4w03Sa6ECH0SZSZBEhUH4kkjuoTHFuxCDQCy5AOxuuzXB3c3JxBDIXVgpv/7hnH88udz3f297vPffePR/Jze73e77n3Hdzs/u833PP93NSVUiSBPA94x5AkrR2GAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1Gwe9wDLdcEFF9T27dvHPYYkrSuf/exnv1pVk6OOW3dR2L59OzMzM+MeQ5LWlSRf7nKcTx9JkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpGbdXbwmSSvtmmuu4b777uPCCy/khhtuGPc4Y2UUJD3m3Xfffdxzzz3jHmNN8OkjSVJjFCRJjVGQJDVGQZLUGAVJUmMUJElNr1FIsjvJySSzSa5d4vYrkswnuWPw8St9ziNJOrverlNIsgk4BFwKzAHHkkxX1YlFh76/qg70NYckqbs+zxQuBmar6lRVnQaOAHt7/HqSpPPUZxS2AHcPbc8N9i3280k+n+RDSbb1OI8kaYQ+o5Al9tWi7ZuB7VX1AuDjwLuWfKBkf5KZJDPz8/MrPKYk6WF9RmEOGP7Nfytw7/ABVfU/VfXQYPMdwEVLPVBVHa6qqaqampyc7GVYSVK/C+IdA3Ym2QHcA+wDXj18QJJnVNV/DTb3AHf2OM+a50qN65ffO20UvUWhqs4kOQDcAmwCbqqq40kOAjNVNQ1clWQPcAa4H7iir3nWA1dqXL/83j3S066+etwjLMuu+XkeD/zH/Py6mv1rN9644o/Z69LZVXUUOLpo33VDn78ReGOfM0iSuvOKZklSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlS0+sqqdL5WE9LGLv0sjaKDR2F9fQ/J/iDRRqX0xMTj/jvY9mGjoIkdTH7rGeNe4Q1w78pSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYF8aQV4Cqb2iiMgrQCXGVTG0WvTx8l2Z3kZJLZJNee5bjLklSSqT7nWetOT0zwzYkJf9uUNDa9nSkk2QQcAi4F5oBjSaar6sSi454MXAV8uq9Z1gt/25Q0bn2eKVwMzFbVqao6DRwB9i5x3PXADcA3e5xFktRBn1HYAtw9tD032NckeRGwrao+0uMckqSO+oxClthX7cbke4Abgd8e+UDJ/iQzSWbm5+dXcERJ0rA+ozAHbBva3grcO7T9ZOD5wG1J7gJ+DJhe6o/NVXW4qqaqampycrLHkSXpsa3PKBwDdibZkWQC2AdMP3xjVX29qi6oqu1VtR34FLCnqmZ6nEmSdBa9RaGqzgAHgFuAO4EPVNXxJAeT7Onr60qSzl2vF69V1VHg6KJ91z3KsZf0OYskaTTXPpIkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSYxQkSY1RkCQ1RkGS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNUZBktQYBUlSMzIKSTYl+fhqDCNJGq+RUaiqbwMPJnnqKswjSRqjzR2P+ybwhST/CPzvwzur6qpeppIkjUXXKHx08LEsSXYDfw5sAv66qv540e2/BlwJfBt4ANhfVSeW+3UkSSujUxSq6l1JngD8cFWd7HKfJJuAQ8ClwBxwLMn0oh/6762qvxocvwf4M2D3cv4BkqSV0+nVR0l+DrgD+Nhg+4VJpkfc7WJgtqpOVdVp4Aiwd/iAqvrG0OYTgeo6uCRp5XV9+ugPWPghfxtAVd2RZMeI+2wB7h7angN+dPFBSa4E3gBMAC/tOI8kqQddr1M4U1VfX7Rv1G/1WWLfd92nqg5V1bOB3wV+b8kHSvYnmUkyMz8/32lgSdLydY3CF5O8GtiUZGeSvwA+OeI+c8C2oe2twL1nOf4I8Mqlbqiqw1U1VVVTk5OTHUeWJC1X1yj8BvA84CHgfcA3gN8acZ9jwM4kO5JMAPuAR/wdIsnOoc2fBb7UcR5JUg+6vvroQeDNg49OqupMkgPALSy8JPWmqjqe5CAwU1XTwIEkLwe+BXwNeM1y/wGSpJVz1igkuZmz/O2gqvac7f5VdRQ4umjfdUOf/2a3MSVJq2HUmcJbB/99FXAh8J7B9uXAXT3NJEkak7NGoapuB0hyfVW9ZOimm5N8otfJJEmrrusfmieTPOvhjcE1Cr4MSJI2mK4Xr10N3Jbk1GB7O/C6XiaSJI1N11cffWzw8tHnDnb9e1U91N9YkqRx6HqmAHARC2cIm4EfSUJV/W0vU0mSxqJTFJK8G3g2C4vifXuwuwCjIEkbSNczhSlgV1W5iqkkbWCd1z5i4ToFSdIG1vVM4QLgRJLPsLD+ETD6imZJ0vqynPdTkCRtcF1fknp7kmcCO6vq40m+j4VF7iRJG0jXt+P8VeBDwNsHu7YAH+5rKEnSeHT9Q/OVwE+y8D4KVNWXgKf3NZQkaTy6RuGhqjr98EaSzYx+O05J0jrTNQq3J3kT8IQklwIfBG7ubyxJ0jh0jcK1wDzwBWA/8NGq6vwubJKk9eGsUUiyN8mVVfWdqnoH8EwWrm5+U5LLVmVCSdKqGXWmcA0wPbQ9wcLCeJcAv97TTJKkMRl1ncJEVd09tP0vVXU/cH+SJ/Y4lyRpDEadKTxteKOqDgxt+s5rkrTBjIrCpwcXrj1CktcBn+lnJEnSuIx6+uhq4MNJXg18brDvIuBxwCv7HEyStPrOGoWq+grwE0leCjxvsPujVXVr75NJklZd1wXxbgUMgSRtcF0vXpMkPQYYBUlSYxQkSY1RkCQ1RkGS1BgFSVLTaxSS7E5yMslskmuXuP0NSU4k+XySfxq8D7QkaUx6i0KSTcAh4BXALuDyJLsWHfZvwFRVvYCF94C+oa95JEmj9XmmcDEwW1WnBm/leQTYO3xAVf1zVT042PwUsLXHeSRJI/QZhS3A8LLbc4N9j+a1wN8vdUOS/UlmkszMz8+v4IiSpGF9RiFL7KslD0x+kYV3dHvLUrdX1eGqmqqqqclJV+yWpL50WvvoHM0B24a2twL3Lj4oycuBNwM/XVUP9TiPJGmEPs8UjgE7k+xIMgHs45Fv7UmSFwFvB/YMVmSVJI1Rb1GoqjPAAeAW4E7gA1V1PMnBJHsGh70FeBLwwSR3JJl+lIeTJK2CPp8+oqqOAkcX7btu6POX9/n1JUnL4xXNkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJKaXqOQZHeSk0lmk1y7xO0vSfK5JGeSXNbnLJKk0XqLQpJNwCHgFcAu4PIkuxYd9p/AFcB7+5pDktTd5h4f+2JgtqpOASQ5AuwFTjx8QFXdNbjtOz3OIUnqqM+nj7YAdw9tzw32SZLWqD6jkCX21Tk9ULI/yUySmfn5+fMcS5L0aPqMwhywbWh7K3DvuTxQVR2uqqmqmpqcnFyR4SRJ363PKBwDdibZkWQC2AdM9/j1JEnnqbcoVNUZ4ABwC3An8IGqOp7kYJI9AElenGQO+AXg7UmO9zWPJGm0Pl99RFUdBY4u2nfd0OfHWHhaSZK0BnhFsySpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkxihIkhqjIElqjIIkqTEKkqTGKEiSGqMgSWqMgiSpMQqSpMYoSJIaoyBJaoyCJKkxCpKkptcoJNmd5GSS2STXLnH745K8f3D7p5Ns73MeSdLZ9RaFJJuAQ8ArgF3A5Ul2LTrstcDXquo5wI3An/Q1jyRptD7PFC4GZqvqVFWdBo4Aexcdsxd41+DzDwEvS5IeZ5IknUWfUdgC3D20PTfYt+QxVXUG+DrwAz3OJEk6i809PvZSv/HXORxDkv3A/sHmA0lOnudsa9kFwFfHPcRy5G1vG/cIa4Xfu/Vto3//ntnloD6jMAdsG9reCtz7KMfMJdkMPBW4f/EDVdVh4HBPc64pSWaqamrcc2j5/N6tb37/FvT59NExYGeSHUkmgH3A9KJjpoHXDD6/DLi1qr7rTEGStDp6O1OoqjNJDgC3AJuAm6rqeJKDwExVTQPvBN6dZJaFM4R9fc0jSRot/mK+tiTZP3i6TOuM37v1ze/fAqMgSWpc5kKS1BiFNSLJc5P8a5KHkvzOuOfR8oxa0kVrV5KbknwlyRfHPctaYBTWjvuBq4C3jnsQLU/HJV20dv0NsHvcQ6wVRmGNqKqvVNUx4FvjnkXL1mVJF61RVfUJlrg+6rHKKEjnr8uSLtK6YBSk89dpuRZpPTAKY5TkyiR3DD5+aNzz6Jx1WdJFWheMwhhV1aGqeuHgwx8i61eXJV2kdcGL19aIJBcCM8BTgO8ADwC7quobYx1MnST5GeBt/P+SLn805pHUUZL3AZewsErqfwO/X1XvHOtQY2QUJEmNTx9JkhqjIElqjIIkqTEKkqTGKEiSGqMgrYAkT0lyT5K/XOK2aVfg1HphFKSVcT1w++KdSV7FwjUn0rpgFKSOkrw4yeeTPD7JE5McT/L8JBcBPwj8w6LjnwS8AfjDccwrnYvN4x5AWi+q6liSaRZ+yD8BeA9wArgV+CXgZYvucj3wp8CDqzmndD48U5CW5yBwKTAF3AC8HjhaVcNLZ5PkhcBzqurvVn9E6dx5piAtz/cDTwK+F3g88OPATyV5/WD/RJIHgC8DFyW5i4X/z56e5LaqumQsU0sdufaRtAyDp4+OADuAZ1TVgaHbrgCmhvcN9m8HPlJVz1+9SaVz45mC1FGSXwbOVNV7B+/L/MkkL62qW8c9m7RSPFOQJDX+oVmS1BgFSVJjFCRJjVGQJDVGQZLUGAVJUmMUJEmNUZAkNf8HW4fLaIq10PIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('x44', 'Gender', data=df, color=\"teal\") #var 44 doesn't capture gender\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22a7f99c518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('x43', 'Gender', data=df, color=\"teal\") #var 43 varies depending on gender -will consider variable below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22a7f9f0d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('x42', 'Gender', data=df, color=\"teal\") #the same as x44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x288d0df9668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot('x45', 'Gender', data=df, color=\"teal\")  #the same as x44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x43_0</th>\n",
       "      <th>x43_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.034083</td>\n",
       "      <td>5.018200</td>\n",
       "      <td>4.911232</td>\n",
       "      <td>4.912412</td>\n",
       "      <td>4.956377</td>\n",
       "      <td>5.024579</td>\n",
       "      <td>4.935966</td>\n",
       "      <td>5.067840</td>\n",
       "      <td>5.086828</td>\n",
       "      <td>5.010133</td>\n",
       "      <td>...</td>\n",
       "      <td>4.988960</td>\n",
       "      <td>4.988325</td>\n",
       "      <td>4.985159</td>\n",
       "      <td>4.984694</td>\n",
       "      <td>4.908684</td>\n",
       "      <td>4.868374</td>\n",
       "      <td>4.995217</td>\n",
       "      <td>5.165625</td>\n",
       "      <td>0.337000</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.909781</td>\n",
       "      <td>2.889966</td>\n",
       "      <td>2.871402</td>\n",
       "      <td>2.925843</td>\n",
       "      <td>2.897600</td>\n",
       "      <td>2.913924</td>\n",
       "      <td>2.921489</td>\n",
       "      <td>2.833147</td>\n",
       "      <td>2.850428</td>\n",
       "      <td>2.911342</td>\n",
       "      <td>...</td>\n",
       "      <td>2.850379</td>\n",
       "      <td>2.853574</td>\n",
       "      <td>2.906113</td>\n",
       "      <td>2.931335</td>\n",
       "      <td>2.848105</td>\n",
       "      <td>2.852245</td>\n",
       "      <td>2.911902</td>\n",
       "      <td>2.803282</td>\n",
       "      <td>0.472803</td>\n",
       "      <td>0.467929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>0.006314</td>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.008044</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.009028</td>\n",
       "      <td>0.005836</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.006052</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.460645</td>\n",
       "      <td>2.535577</td>\n",
       "      <td>2.503660</td>\n",
       "      <td>2.323453</td>\n",
       "      <td>2.471000</td>\n",
       "      <td>2.495113</td>\n",
       "      <td>2.334584</td>\n",
       "      <td>2.576546</td>\n",
       "      <td>2.600456</td>\n",
       "      <td>2.559807</td>\n",
       "      <td>...</td>\n",
       "      <td>2.634839</td>\n",
       "      <td>2.468765</td>\n",
       "      <td>2.533647</td>\n",
       "      <td>2.426546</td>\n",
       "      <td>2.494812</td>\n",
       "      <td>2.383270</td>\n",
       "      <td>2.489068</td>\n",
       "      <td>2.798774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.059938</td>\n",
       "      <td>5.024768</td>\n",
       "      <td>4.805803</td>\n",
       "      <td>4.806753</td>\n",
       "      <td>4.924308</td>\n",
       "      <td>5.020893</td>\n",
       "      <td>4.964938</td>\n",
       "      <td>5.193205</td>\n",
       "      <td>5.201639</td>\n",
       "      <td>4.975510</td>\n",
       "      <td>...</td>\n",
       "      <td>4.925258</td>\n",
       "      <td>4.936059</td>\n",
       "      <td>4.978506</td>\n",
       "      <td>5.042467</td>\n",
       "      <td>4.883605</td>\n",
       "      <td>4.776674</td>\n",
       "      <td>5.024223</td>\n",
       "      <td>5.159642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.560343</td>\n",
       "      <td>7.571328</td>\n",
       "      <td>7.398937</td>\n",
       "      <td>7.513002</td>\n",
       "      <td>7.496324</td>\n",
       "      <td>7.484141</td>\n",
       "      <td>7.527601</td>\n",
       "      <td>7.497030</td>\n",
       "      <td>7.510152</td>\n",
       "      <td>7.602192</td>\n",
       "      <td>...</td>\n",
       "      <td>7.368070</td>\n",
       "      <td>7.360427</td>\n",
       "      <td>7.509905</td>\n",
       "      <td>7.540748</td>\n",
       "      <td>7.327315</td>\n",
       "      <td>7.278360</td>\n",
       "      <td>7.612702</td>\n",
       "      <td>7.596508</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.997439</td>\n",
       "      <td>9.998735</td>\n",
       "      <td>9.998568</td>\n",
       "      <td>9.991082</td>\n",
       "      <td>9.990208</td>\n",
       "      <td>9.997379</td>\n",
       "      <td>9.999543</td>\n",
       "      <td>9.997743</td>\n",
       "      <td>9.996769</td>\n",
       "      <td>9.993926</td>\n",
       "      <td>...</td>\n",
       "      <td>9.998607</td>\n",
       "      <td>9.999401</td>\n",
       "      <td>9.993760</td>\n",
       "      <td>9.999721</td>\n",
       "      <td>9.998322</td>\n",
       "      <td>9.987979</td>\n",
       "      <td>9.997630</td>\n",
       "      <td>9.987218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x1           x2           x3           x4           x5  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      5.034083     5.018200     4.911232     4.912412     4.956377   \n",
       "std       2.909781     2.889966     2.871402     2.925843     2.897600   \n",
       "min       0.001557     0.006796     0.004259     0.006314     0.009633   \n",
       "25%       2.460645     2.535577     2.503660     2.323453     2.471000   \n",
       "50%       5.059938     5.024768     4.805803     4.806753     4.924308   \n",
       "75%       7.560343     7.571328     7.398937     7.513002     7.496324   \n",
       "max       9.997439     9.998735     9.998568     9.991082     9.990208   \n",
       "\n",
       "                x6           x7           x8           x9          x10  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      5.024579     4.935966     5.067840     5.086828     5.010133   \n",
       "std       2.913924     2.921489     2.833147     2.850428     2.911342   \n",
       "min       0.002398     0.003581     0.002622     0.008044     0.001992   \n",
       "25%       2.495113     2.334584     2.576546     2.600456     2.559807   \n",
       "50%       5.020893     4.964938     5.193205     5.201639     4.975510   \n",
       "75%       7.484141     7.527601     7.497030     7.510152     7.602192   \n",
       "max       9.997379     9.999543     9.997743     9.996769     9.993926   \n",
       "\n",
       "          ...               x33          x34          x35          x36  \\\n",
       "count     ...       2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      ...          4.988960     4.988325     4.985159     4.984694   \n",
       "std       ...          2.850379     2.853574     2.906113     2.931335   \n",
       "min       ...          0.000834     0.009028     0.005836     0.002803   \n",
       "25%       ...          2.634839     2.468765     2.533647     2.426546   \n",
       "50%       ...          4.925258     4.936059     4.978506     5.042467   \n",
       "75%       ...          7.368070     7.360427     7.509905     7.540748   \n",
       "max       ...          9.998607     9.999401     9.993760     9.999721   \n",
       "\n",
       "               x37          x38          x39          x40        x43_0  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      4.908684     4.868374     4.995217     5.165625     0.337000   \n",
       "std       2.848105     2.852245     2.911902     2.803282     0.472803   \n",
       "min       0.000178     0.006052     0.000361     0.002939     0.000000   \n",
       "25%       2.494812     2.383270     2.489068     2.798774     0.000000   \n",
       "50%       4.883605     4.776674     5.024223     5.159642     0.000000   \n",
       "75%       7.327315     7.278360     7.612702     7.596508     1.000000   \n",
       "max       9.998322     9.987979     9.997630     9.987218     1.000000   \n",
       "\n",
       "             x43_1  \n",
       "count  2000.000000  \n",
       "mean      0.323500  \n",
       "std       0.467929  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "X=df.copy()\n",
    "\n",
    "#drop dependent variable and categorical ones as described above\n",
    "X.drop(['Gender'], axis=1, inplace=True)\n",
    "X.drop(['x41'], axis=1, inplace=True)\n",
    "X.drop(['x42'], axis=1, inplace=True)\n",
    "X.drop(['x44'], axis=1, inplace=True)\n",
    "X.drop(['x45'], axis=1, inplace=True)\n",
    "X=pd.get_dummies(X, columns=['x43'])\n",
    "X.drop(['x43_-1'], axis=1, inplace=True)\n",
    "y = df['Gender']\n",
    "#copy X for recursive feature elimination\n",
    "X_rfe=X.copy()\n",
    "\n",
    "#split on subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "#check if everything is fine\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000337764364541264\n",
      "x1       0.000000\n",
      "x2      -0.000000\n",
      "x3       0.035548\n",
      "x4      -0.000000\n",
      "x5       0.000000\n",
      "x6       0.000000\n",
      "x7       0.000000\n",
      "x8      -0.000000\n",
      "x9       0.001220\n",
      "x10     -0.000000\n",
      "x11     -0.000000\n",
      "x12     -0.027718\n",
      "x13      0.000000\n",
      "x14      0.000733\n",
      "x15     -0.000000\n",
      "x16      0.000000\n",
      "x17      0.000000\n",
      "x18      0.002563\n",
      "x19     -0.000000\n",
      "x20     -0.000000\n",
      "x21     -0.030908\n",
      "x22      0.000000\n",
      "x23      0.000000\n",
      "x24     -0.000000\n",
      "x25     -0.000000\n",
      "x26      0.000000\n",
      "x27     -0.000000\n",
      "x28      0.000000\n",
      "x29     -0.000000\n",
      "x30     -0.000000\n",
      "x31      0.000000\n",
      "x32      0.000000\n",
      "x33     -0.000000\n",
      "x34      0.000000\n",
      "x35     -0.000000\n",
      "x36      0.000000\n",
      "x37     -0.000000\n",
      "x38      0.000281\n",
      "x39     -0.000000\n",
      "x40     -0.000000\n",
      "x43_0    0.105591\n",
      "x43_1    0.309550\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4280880412075506"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\n",
    "lassocv.fit(X_train, y_train)\n",
    "\n",
    "print(lassocv.alpha_)\n",
    "\n",
    "lasso = Lasso(max_iter = 10000, normalize = True)\n",
    "lasso.set_params(alpha=lassocv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, lasso.predict(X_test))\n",
    "\n",
    "lasso.fit(X, y)\n",
    "print(pd.Series(lasso.coef_, index = X.columns))\n",
    "lasso.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit with Lasso variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split results:\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "       x43_0     x43_1       x38       x21       x18       x14       x12  \\\n",
      "0  0.767395  1.958576  0.028076 -0.189053  0.031119  0.022651 -0.186383   \n",
      "\n",
      "         x9        x3  intercept  \n",
      "0  0.028044  0.234555  -0.882763  \n",
      "LogisticRegression accuracy is 0.667\n",
      "LogisticRegression precision is 0.667\n",
      "LogisticRegression recall is 0.587\n",
      "LogisticRegression auc is 0.745\n"
     ]
    }
   ],
   "source": [
    "#non-zero variables\n",
    "Selected_features = ['x43_0','x43_1','x38','x21','x18','x14','x12','x9','x3']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# select only important variables\n",
    "X_l = X[Selected_features]\n",
    "\n",
    "#split on train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_l, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# build logit\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "colum_label = list(X_train.columns)\n",
    "model_Coeff = pd.DataFrame(logreg.coef_, columns = colum_label)\n",
    "model_Coeff['intercept'] = logreg.intercept_\n",
    "\n",
    "# results\n",
    "print('Test split results:')\n",
    "\n",
    "print(\"Coefficient Values Of The Surface Are: \\n\\n\", model_Coeff)\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" precision is %2.3f\" % precision_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" recall is %2.3f\" % recall_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit with Lasso (standartized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split results:\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "       x43_0     x43_1       x38       x21       x18       x14       x12  \\\n",
      "0  0.379723  0.950424  0.087366 -0.531168  0.098802  0.071424 -0.527397   \n",
      "\n",
      "         x9        x3  intercept  \n",
      "0  0.087944  0.689762  -0.186923  \n",
      "LogisticRegression accuracy is 0.670\n",
      "LogisticRegression precision is 0.672\n",
      "LogisticRegression recall is 0.587\n",
      "LogisticRegression auc is 0.745\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "#the same variables from Lasso\n",
    "Selected_features = ['x43_0','x43_1','x38','x21','x18','x14','x12','x9','x3']\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_ls = X[Selected_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ls, y, test_size=0.3, random_state=2)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "#standartization\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "#build logit\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_std, y_train)\n",
    "y_pred = logreg.predict(X_test_std)\n",
    "y_pred_proba = logreg.predict_proba(X_test_std)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "colum_label = list(X_train.columns)\n",
    "model_Coeff = pd.DataFrame(logreg.coef_, columns = colum_label)\n",
    "model_Coeff['intercept'] = logreg.intercept_\n",
    "\n",
    "#results\n",
    "print('Test split results:')\n",
    "\n",
    "print(\"Coefficient Values Of The Surface Are: \\n\\n\", model_Coeff)\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" precision is %2.3f\" % precision_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" recall is %2.3f\" % recall_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit with Lasso (ln variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split results:\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "       x43_0     x43_1       x38       x21       x18      x14       x12  \\\n",
      "0  0.722856  1.850217  0.100884 -0.477309  0.104674  0.09266 -0.474005   \n",
      "\n",
      "         x9        x3  intercept  \n",
      "0  0.010732  0.515969  -0.805549  \n",
      "LogisticRegression accuracy is 0.653\n",
      "LogisticRegression precision is 0.647\n",
      "LogisticRegression recall is 0.583\n",
      "LogisticRegression auc is 0.724\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None  \n",
    "\n",
    "Selected_features = ['x43_0','x43_1','x38','x21','x18','x14','x12','x9','x3']\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create variables with ln\n",
    "X_ln = X[Selected_features]\n",
    "X_ln['x38']=np.log(X['x38']+0.000000001)\n",
    "X_ln['x21']=np.log(X['x21']+0.000000001)\n",
    "X_ln['x18']=np.log(X['x18']+0.000000001)\n",
    "X_ln['x14']=np.log(X['x14']+0.000000001)\n",
    "X_ln['x12']=np.log(X['x12']+0.000000001)\n",
    "X_ln['x9']=np.log(X['x9']+0.000000001)\n",
    "X_ln['x3']=np.log(X['x3']+0.000000001)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ln, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# build logit\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "colum_label = list(X_train.columns)\n",
    "model_Coeff = pd.DataFrame(logreg.coef_, columns = colum_label)\n",
    "model_Coeff['intercept'] = logreg.intercept_\n",
    "\n",
    "#results\n",
    "print('Test split results:')\n",
    "\n",
    "print(\"Coefficient Values Of The Surface Are: \\n\\n\", model_Coeff)\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" precision is %2.3f\" % precision_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" recall is %2.3f\" % recall_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination (7-12 variables)+ Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x3', 'x9', 'x12', 'x18', 'x21', 'x43_0', 'x43_1']\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x21', 'x43_0', 'x43_1']\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x21', 'x38', 'x43_0', 'x43_1']\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x20', 'x21', 'x38', 'x43_0', 'x43_1']\n",
      "['x3', 'x9', 'x11', 'x12', 'x14', 'x18', 'x20', 'x21', 'x38', 'x43_0', 'x43_1']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "X_r = X_rfe\n",
    "y_r = y\n",
    "\n",
    "sel=[]\n",
    "\n",
    "#select list of list important variables - from 7 to 12\n",
    "for i in range (7,12):\n",
    "    model = LogisticRegression()\n",
    "    rfe = RFE(model,i)\n",
    "    rfe = rfe.fit(X_r, y_r)\n",
    "    print(list(X.columns[rfe.support_]))\n",
    "    sel.append (list(X.columns[rfe.support_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split results:\n",
      "['x3', 'x9', 'x12', 'x18', 'x21', 'x43_0', 'x43_1']\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "          x3        x9       x12     x18      x21     x43_0     x43_1  \\\n",
      "0  0.234198  0.028311 -0.186698  0.0313 -0.18891  0.767553  1.962431   \n",
      "\n",
      "   intercept  \n",
      "0  -0.631173  \n",
      "LogisticRegression accuracy is 0.665\n",
      "LogisticRegression precision is 0.667\n",
      "LogisticRegression recall is 0.580\n",
      "LogisticRegression auc is 0.742\n",
      "---------------------------------------------\n",
      "Test split results:\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x21', 'x43_0', 'x43_1']\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "          x3        x9       x12       x14       x18      x21     x43_0  \\\n",
      "0  0.234489  0.027989 -0.185994  0.022246  0.030682 -0.19022  0.762308   \n",
      "\n",
      "      x43_1  intercept  \n",
      "0  1.961384  -0.735864  \n",
      "LogisticRegression accuracy is 0.660\n",
      "LogisticRegression precision is 0.657\n",
      "LogisticRegression recall is 0.583\n",
      "LogisticRegression auc is 0.744\n",
      "---------------------------------------------\n",
      "Test split results:\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x21', 'x38', 'x43_0', 'x43_1']\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "          x3        x9       x12       x14       x18      x21       x38  \\\n",
      "0  0.234552  0.028044 -0.186377  0.022653  0.031119 -0.18905  0.028078   \n",
      "\n",
      "      x43_0     x43_1  intercept  \n",
      "0  0.767385  1.958568  -0.882801  \n",
      "LogisticRegression accuracy is 0.667\n",
      "LogisticRegression precision is 0.667\n",
      "LogisticRegression recall is 0.587\n",
      "LogisticRegression auc is 0.745\n",
      "---------------------------------------------\n",
      "Test split results:\n",
      "['x3', 'x9', 'x12', 'x14', 'x18', 'x20', 'x21', 'x38', 'x43_0', 'x43_1']\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "          x3        x9       x12      x14       x18       x20       x21  \\\n",
      "0  0.234515  0.027987 -0.186249  0.02273  0.030912 -0.004792 -0.189398   \n",
      "\n",
      "        x38     x43_0     x43_1  intercept  \n",
      "0  0.028086  0.768637  1.959926  -0.857827  \n",
      "LogisticRegression accuracy is 0.670\n",
      "LogisticRegression precision is 0.671\n",
      "LogisticRegression recall is 0.590\n",
      "LogisticRegression auc is 0.745\n",
      "---------------------------------------------\n",
      "Test split results:\n",
      "['x3', 'x9', 'x11', 'x12', 'x14', 'x18', 'x20', 'x21', 'x38', 'x43_0', 'x43_1']\n",
      "Coefficient Values Of The Surface Are: \n",
      "\n",
      "          x3        x9       x11       x12       x14       x18       x20  \\\n",
      "0  0.234518  0.027488 -0.020651 -0.186596  0.021796  0.031395 -0.004951   \n",
      "\n",
      "        x21       x38     x43_0     x43_1  intercept  \n",
      "0 -0.189049  0.028861  0.770957  1.964954   -0.75521  \n",
      "LogisticRegression accuracy is 0.670\n",
      "LogisticRegression precision is 0.669\n",
      "LogisticRegression recall is 0.594\n",
      "LogisticRegression auc is 0.745\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, average_precision_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "\n",
    "#build logits with all selected sets of variables\n",
    "for l in sel:\n",
    "    Selected_features = l\n",
    "\n",
    "    X_rl = X_r[Selected_features]\n",
    "    y = y\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_rl, y, test_size=0.3, random_state=2)\n",
    "\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "    [fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "    \n",
    "    colum_label = list(X_train.columns)\n",
    "    model_Coeff = pd.DataFrame(logreg.coef_, columns = colum_label)\n",
    "    model_Coeff['intercept'] = logreg.intercept_\n",
    "\n",
    "    print('Test split results:')\n",
    "    print (l)\n",
    "    print(\"Coefficient Values Of The Surface Are: \\n\\n\", model_Coeff)\n",
    "    print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "    print(logreg.__class__.__name__+\" precision is %2.3f\" % precision_score(y_test, y_pred))\n",
    "    print(logreg.__class__.__name__+\" recall is %2.3f\" % recall_score(y_test, y_pred))\n",
    "    print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "    print (\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest+Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6316666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Selected_features = ['x43_0','x43_1','x38','x21','x18','x14','x12','x9','x3']\n",
    "X_f=X[Selected_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_f, y, test_size=0.3, random_state=2)\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes+PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "Selected_features = ['x43_0','x43_1','x38','x21','x18','x14','x12','x9','x3']\n",
    "X_n=X[Selected_features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_n, y, test_size=0.3, random_state=2)\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_std = std_scale.transform(X_train)\n",
    "X_test_std = std_scale.transform(X_test)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# on non-standardized data\n",
    "pca = PCA(n_components=2).fit(X_train)\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "# om standardized data\n",
    "pca_std = PCA(n_components=2).fit(X_train_std)\n",
    "X_train_std = pca_std.transform(X_train_std)\n",
    "X_test_std = pca_std.transform(X_test_std)\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# on non-standardized data\n",
    "gnb = GaussianNB()\n",
    "fit = gnb.fit(X_train, y_train)\n",
    "\n",
    "# on standardized data\n",
    "gnb_std = GaussianNB()\n",
    "fit_std = gnb_std.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy for the training dataset\n",
      "0.5635714285714286\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.5266666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pred_train = gnb.predict(X_train)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print(metrics.accuracy_score(y_train, pred_train))\n",
    "\n",
    "pred_test = gnb.predict(X_test)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print(metrics.accuracy_score(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction accuracy for the training dataset\n",
      "0.6542857142857142\n",
      "\n",
      "Prediction accuracy for the test dataset\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "pred_train_std = gnb_std.predict(X_train_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the training dataset')\n",
    "print(metrics.accuracy_score(y_train, pred_train_std))\n",
    "\n",
    "pred_test_std = gnb_std.predict(X_test_std)\n",
    "\n",
    "print('\\nPrediction accuracy for the test dataset')\n",
    "print(metrics.accuracy_score(y_test, pred_test_std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
